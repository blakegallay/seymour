{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "from newspaper import Article\n",
    "\n",
    "from newspaper3k_fixes import get_authors, get_keywords\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here I'm just experimenting with the text in articles, seeing what can be done with simple algorithms/API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles:\n",
      "\n",
      "0 : Supreme Court blocks Trump from ending DACA\n",
      "1 : Biden calls Supreme Court DACA ruling a ‘victory,’ vows to make program ‘permanent’\n",
      "2 : Guy Benson rips Supreme Court DACA decision as a 'travesty that could set a bad precedent'\n",
      "3 : It’s Time to End DACA – It’s Unconstitutional Unless Approved by Congress\n",
      "4 : Trump lashes out at Supreme Court after DACA ruling doesn't go his way\n"
     ]
    }
   ],
   "source": [
    "# A variety of articles with similar/different viewpoints and topics\n",
    "urls = [\n",
    "    # (CNN) Breaking news report on the Supreme Court's DACA decision\n",
    "    'https://www.cnn.com/2020/06/18/politics/daca-immigration-supreme-court/index.html',\n",
    "\n",
    "    # (Fox News) Report on Biden's statement on the decision, which he strongly celebrated\n",
    "    'https://www.foxnews.com/politics/biden-calls-supreme-court-daca-ruling-a-victory-vows-to-make-program-permanent',\n",
    "\n",
    "    # (Fox News) Fox News columnist calling the decision a 'travesty'\n",
    "    'https://www.foxnews.com/media/guy-benson-supreme-court-daca-decision-travesty',\n",
    "\n",
    "    # (The Heritage Foundation) From a conservative foundation, arguing that DACA is unconstitutional\n",
    "    'https://www.heritage.org/courts/commentary/its-time-end-daca-its-unconstitutional-unless-approved-congress',\n",
    "\n",
    "    # (MSNBC) Report on Trump's reaction, described as 'lashing out' after it doesn't 'go his way'\n",
    "    'https://www.nbcnews.com/politics/donald-trump/trump-lashes-out-supreme-court-after-daca-ruling-doesn-t-n1231438',\n",
    "]\n",
    "\n",
    "articles = list(map(Article, urls))\n",
    "for article in articles:\n",
    "    Article.download(article)\n",
    "    article.parse()\n",
    "    \n",
    "print(\"Titles:\\n\")\n",
    "for i, article in enumerate(articles):\n",
    "    print(f\"{i} : {article.title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "for article in articles:\n",
    "    keywords[article] = get_keywords(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['supreme', 'court', 'blocks', 'trump', 'from ending', 'daca']\n",
      "['biden calls', 'supreme', 'court', 'daca', 'ruling', 'a ‘victory,’ vows to make', 'program', '‘permanent’']\n",
      "['guy', 'benson', 'rips supreme court', 'daca', 'decision', \"as a 'travesty that could set a bad precedent'\"]\n",
      "['it’s time to end', 'daca', '– it’s unconstitutional unless approved by', 'congress']\n",
      "['trump', 'lashes out at', 'supreme', 'court', 'after', 'daca', 'ruling', \"doesn't go his way\"]\n"
     ]
    }
   ],
   "source": [
    "# Split title on keywords\n",
    "for article in articles:\n",
    "    split_title = []\n",
    "\n",
    "    acc = []\n",
    "    for word in article.title.lower().split(\" \"):\n",
    "        if word in keywords[article]['text']:\n",
    "            if acc != []:\n",
    "                split_title.append(\" \".join(acc))\n",
    "            split_title.append(word)\n",
    "            acc = []\n",
    "        else:\n",
    "            acc.append(word)\n",
    "            \n",
    "    if(acc != []):\n",
    "        split_title.append(\" \".join(acc))\n",
    "\n",
    "    print(split_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<newspaper.article.Article object at 0x7fd62d0f3520>, <newspaper.article.Article object at 0x7fd62d0f3c10>, <newspaper.article.Article object at 0x7fd62d0f3490>, <newspaper.article.Article object at 0x7fd62d0f3f70>, <newspaper.article.Article object at 0x7fd62d0f3970>]\n"
     ]
    }
   ],
   "source": [
    "print(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title stuff and proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://www.foxnews.com/politics/biden-calls-supreme-court-daca-ruling-a-victory-vows-to-make-program-permanent\n",
      "\n",
      "words in the title: ['Biden', 'calls', 'Supreme', 'Court', 'DACA', 'ruling', 'a', '‘victory,’', 'vows', 'to', 'make', 'program', '‘permanent’']\n",
      "\n",
      "start w/ uppercase in title: ['Biden', 'Supreme', 'Court', 'DACA']\n",
      "\n",
      "usually start w/ uppercase in body: ['Vice', 'President', 'Joe', 'Biden', 'Supreme', 'Court', 'Thursday', 'Trump', 'Obama-era', 'Court’s', 'Democratic', 'White', 'House', 'Congress', 'Chief', 'Justice', 'John', 'Roberts', 'Deferred', 'Action', 'Childhood', 'Arrivals', 'Administrative', 'Procedure', 'Act', 'Barack', 'Obama', 'American', \"I'm\", 'Obama’s', 'November', 'Monday', 'Republicans', 'Conservatives”', 'Election', 'Day', 'Justices', 'United', 'States', 'News’', 'Bill', 'Mears', 'Ronn', 'Blitzer', 'Madeleine', 'Rivera']\n",
      "\n",
      "naive combining propers: ['Biden', 'calls', 'Supreme Court', 'DACA', 'ruling', 'a', '‘victory,’', 'vows', 'to', 'make', 'program', '‘permanent’']\n",
      "\n",
      "commonly paired propers: [('Vice', 'President', 'Joe', 'Biden'), ('President', 'Joe', 'Biden'), ('Joe', 'Biden'), ('Supreme', 'Court'), ('President', 'Trump'), ('White', 'House'), ('Chief', 'Justice', 'John', 'Roberts'), ('Justice', 'John', 'Roberts'), ('John', 'Roberts'), ('Deferred', 'Action'), ('Childhood', 'Arrivals'), ('Administrative', 'Procedure', 'Act'), ('Procedure', 'Act'), ('President', 'Barack', 'Obama'), ('Barack', 'Obama'), ('Conservatives”', 'Trump'), ('Election', 'Day'), ('United', 'States'), ('Bill', 'Mears', 'Ronn', 'Blitzer'), ('Mears', 'Ronn', 'Blitzer'), ('Ronn', 'Blitzer'), ('Madeleine', 'Rivera')]\n",
      "\n",
      "smarter combining propers: ['Biden', 'calls', 'Supreme Court', 'DACA', 'ruling', 'a', '‘victory,’', 'vows', 'to', 'make', 'program', '‘permanent’']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finding proper nouns\n",
    "article = articles[1]\n",
    "print(f\"url: {article.url}\\n\")\n",
    "title_words = article.title.split(\" \")\n",
    "print(f\"words in the title: {title_words}\\n\")\n",
    "\n",
    "# Just looking for anything that starts with a capital letter will\n",
    "# also catch the first word in a headline/sentence.\n",
    "uppers = list(filter(lambda w: w[0].isupper(), title_words))\n",
    "print(f\"start w/ uppercase in title: {uppers}\\n\")\n",
    "\n",
    "# So maybe look in the body and see if the first word in the headline\n",
    "# is usually capitalized or not. Exclude words at the start of sentences.\n",
    "word_counts = {}\n",
    "for sentence in article.text.split('.'):\n",
    "    sentence = sentence.strip('\\n').replace(',', '').split(' ')\n",
    "\n",
    "    if '' in sentence:\n",
    "        sentence.remove('')\n",
    "\n",
    "    for word in sentence[1:]:\n",
    "        if word not in word_counts:\n",
    "            word_counts[word] = {'total': 0, 'starts_with_cap': 0}\n",
    "        word_counts[word]['total'] += 1\n",
    "        if (word[0].isupper() and not all([letter.isupper() for letter in word]) and not \"\\n\" in word):\n",
    "            word_counts[word]['starts_with_cap'] += 1\n",
    "\n",
    "propers = []\n",
    "for word in word_counts:\n",
    "    if word_counts[word]['starts_with_cap'] > math.floor(word_counts[word]['total'] / 2):\n",
    "        propers.append(word)\n",
    "\n",
    "print(f\"usually start w/ uppercase in body: {propers}\\n\")\n",
    "\n",
    "# Combining multiple proper nouns in a row\n",
    "# There are cases where this totally fails\n",
    "split_title = []\n",
    "combo = []\n",
    "for i, word in enumerate(title_words):\n",
    "    if word in propers:\n",
    "        combo.append(word)\n",
    "    elif word not in propers or i == len(title_words):\n",
    "        if(combo != []):\n",
    "            split_title.append(\" \".join(combo))\n",
    "            combo = []\n",
    "        split_title.append(word)\n",
    "\n",
    "print(f\"naive combining propers: {split_title}\\n\")\n",
    "\n",
    "# Partial solution:\n",
    "# only combine propers which are almost always seen in that order together\n",
    "# i.e. 'Supreme Court'\n",
    "combos = {}\n",
    "\n",
    "def remove_punct(s):\n",
    "    # ORDERED list of punctuation to remove\n",
    "    to_remove = [',', '.', \"'s\", \"’s\", \"'\", \"’\"]\n",
    "    for t in to_remove:\n",
    "        s = s.replace(t, '')\n",
    "    return s\n",
    "\n",
    "body_words = article.text.split(\" \")\n",
    "for i, word in enumerate(body_words):\n",
    "    word = remove_punct(word)\n",
    "    combo = []\n",
    "    if word in propers:\n",
    "        for next_word in body_words[i+1:]:\n",
    "            next_word = remove_punct(next_word)\n",
    "            if(next_word in propers):\n",
    "                if(word not in combo):\n",
    "                    combo = [word]\n",
    "                combo.append(next_word)\n",
    "            else:\n",
    "                break\n",
    "        if combo != []:\n",
    "            combo = tuple(combo)\n",
    "            if combo not in combos:\n",
    "                combos[combo] = 1\n",
    "            else:\n",
    "                combos[combo] += 1\n",
    "\n",
    "# This part is a work in progress\n",
    "# The math.floor(.../5) is totally arbitrary, and should be changed\n",
    "to_remove = []\n",
    "for combo, cnt in combos.items():\n",
    "    remove = True\n",
    "    for word in combo:\n",
    "        if cnt > math.floor(word_counts[word]['total'] / 5):\n",
    "            remove = False \n",
    "    if(remove):\n",
    "        to_remove.append(combo)\n",
    "        \n",
    "for combo in to_remove:\n",
    "    del combos[combo]\n",
    "    \n",
    "combos = list(combos.keys())\n",
    "            \n",
    "print(f\"commonly paired propers: {combos}\\n\")\n",
    "\n",
    "# Combining multiple proper nouns in a row (take 2)\n",
    "# Only using the combos in combos\n",
    "split_title = []\n",
    "pieces = []\n",
    "\n",
    "for i, word in enumerate(title_words):\n",
    "    if word in propers:\n",
    "        if all(map(lambda p : (i,word) not in p, pieces)):\n",
    "            piece = [(i, word)]\n",
    "            for j, next_word in enumerate(title_words[i+1:]):\n",
    "                if next_word in propers:\n",
    "                    piece.append((i + 1 + j, next_word))\n",
    "                else:\n",
    "                    break\n",
    "            if(len(piece) == 1):\n",
    "                pieces.append(piece)\n",
    "            else:\n",
    "                is_combo = False\n",
    "                for k in range(len(piece)-1):\n",
    "                    if tuple(map(lambda w : w[1], piece)) in combos:\n",
    "                        pieces.append(piece)\n",
    "                        is_combo = True\n",
    "                        break\n",
    "                    else:\n",
    "                        del piece[-1]\n",
    "                if not is_combo:\n",
    "                    pieces.append([piece[0]])\n",
    "    else:\n",
    "        pieces.append([(i,word)])\n",
    "\n",
    "# Put the pieces back together\n",
    "split_title2 = list(map(lambda p : \" \".join(tuple(map(lambda w : w[1], p))), pieces))\n",
    "print(f\"smarter combining propers: {split_title2}\\n\")\n",
    "\n",
    "# Next: combine non-proper nouns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLK (Natural Language Toolkit) Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/blake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/blake/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/blake/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "amazing:v\n",
      "interesting:v\n",
      "great:n\n",
      "blocks:n\n"
     ]
    }
   ],
   "source": [
    "# Identifying type of words\n",
    "words = ['amazing', 'interesting', 'great', 'blocks']\n",
    "for w in words:\n",
    "    tmp = wn.synsets(w)[0].pos()\n",
    "    print(w + \":\" + tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden\n",
      "calls  :  call\n",
      "Supreme Court\n",
      "DACA\n",
      "ruling  :  rule\n",
      "a  :  a\n",
      "‘victory,’  :  ‘victory,’\n",
      "vows  :  vow\n",
      "to  :  to\n",
      "make  :  make\n",
      "program  :  program\n",
      "‘permanent’  :  ‘permanent’\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer() \n",
    "  \n",
    "# choose some words to be stemmed\n",
    "for p in split_title2: \n",
    "    if(len(p.split(\" \")) == 1 and p not in propers):\n",
    "        print(p, \" : \", ps.stem(p))\n",
    "    else:\n",
    "        print(p)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden\n",
      "calls  :  call\n",
      "Supreme Court\n",
      "DACA\n",
      "ruling  :  ruling\n",
      "a  :  a\n",
      "‘victory,’  :  ‘victory,’\n",
      "vows  :  vow\n",
      "to  :  to\n",
      "make  :  make\n",
      "program  :  program\n",
      "‘permanent’  :  ‘permanent’\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "# choose some words to be lemmatized\n",
    "for p in split_title2: \n",
    "    if(len(p.split(\" \")) == 1 and p not in propers):\n",
    "        print(p, \" : \", lemmatizer.lemmatize(p))\n",
    "    else:\n",
    "        print(p)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Wikipedia articles to get info on proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia!\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden : American politician\n",
      "Supreme Court : highest court within\n",
      "DACA : United States immigration policy\n"
     ]
    }
   ],
   "source": [
    "# Still using the same article from the blocks above\n",
    "terms = list(filter(lambda p: len(p.split(\" \")) > 1 or p in propers, split_title2))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# not stopping at 'and' is sometimes necessary to get a meaningful description\n",
    "stopwords.remove('and')\n",
    "\n",
    "# find the bit of text between the first two stopwords\n",
    "# i.e. \"an American politician who served for 41 years in the Senate\" -> \"American politician\"\n",
    "def condense_desc(desc):\n",
    "    desc_words = desc.split(\" \")\n",
    "    short_description = ''\n",
    "    for i, word in enumerate(desc_words):\n",
    "        if word in stopwords:\n",
    "            for j, next_word in enumerate(desc_words[i+1:]):\n",
    "                if next_word in stopwords:\n",
    "                    return \" \".join(desc_words[i+1:i+1+j])\n",
    "\n",
    "# The first sentence of most Wikipedia articles usually has the following structure:\n",
    "# (topic/title of article) (is/are/was/were) (description).\n",
    "# We want the (description) part of this structure, so we split on the first is/are/was/were.\n",
    "splits = [\"is\", \"are\", \"was\", \"were\"]\n",
    "for term in terms:\n",
    "    article_name = wikipedia.search(term)[0]\n",
    "    summary = wikipedia.summary(article_name)\n",
    "    \n",
    "    first = ''\n",
    "    for word in summary.split(\" \"):\n",
    "        if word in splits:\n",
    "            first = word\n",
    "            break\n",
    "\n",
    "    description = summary.split(first)[1]\n",
    "    short_description = condense_desc(description)\n",
    "\n",
    "    print(f\"{term} : {short_description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Biden', 'calls', 'Supreme Court', 'DACA', 'ruling', 'a', '‘victory,’', 'vows', 'to', 'make', 'program', '‘permanent’']\n"
     ]
    }
   ],
   "source": [
    "print(split_title2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/blake/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden calls Supreme Court DACA ruling a victory, vows to make program permanent\n",
      "{'neg': 0.0, 'neu': 0.753, 'pos': 0.247, 'compound': 0.5574}\n"
     ]
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# must remove quotation marks for the analyzer to work\n",
    "# (NOTE: should do this earlier? proper nouns may show up in quotes...)\n",
    "def remove_quotes(text):\n",
    "    quotes = ['‘', '’', '\"']\n",
    "    for q in quotes:\n",
    "        text = text.replace(q, '')\n",
    "    return text\n",
    "\n",
    "title_noquotes = remove_quotes(\" \".join(title_words))\n",
    "print(title_noquotes)\n",
    "\n",
    "ss1 = sid.polarity_scores(title_noquotes)\n",
    "print(ss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biden calls Supreme Court DACA ruling a victory, vows to make program permanent\n",
      "Approach 1: Biden vows to make program permanent\n",
      "Approach 2: Biden vows to make program permanent\n"
     ]
    }
   ],
   "source": [
    "# News headlines sometimes use a format like this:\n",
    "# [subject] [present tense verb] [...], [another present tense verb] [...]\n",
    "# where the subject is not repeated, and no 'and' is included\n",
    " \n",
    "# To make it easier to compare with other articles, we can try to turn headlines\n",
    "#  with this format into two cohesive sentences.\n",
    "\n",
    "# Find present-tense 3rd person verbs\n",
    "def verb_indices(tokens):\n",
    "    # NLTK's parts-of-speech tagger\n",
    "    # here's a useful page with all the tags: \n",
    "    # https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    result = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Verb, 3rd person singular present\n",
    "    tag = \"VBZ\"\n",
    "\n",
    "    return [i for i in range(len(tokens)) if result[i][1] == tag]\n",
    "\n",
    "text = remove_quotes(article.title)\n",
    "print(text)\n",
    "if text.count(',') == 1:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    v_indices = verb_indices(tokens)\n",
    "    comma_index = tokens.index(',')\n",
    "    \n",
    "    # Check if second part starts with a verb\n",
    "    # the +1 comes from the comma being its own token\n",
    "    if comma_index + 1 in v_indices:\n",
    "        first_index = min(v_indices)\n",
    "        \n",
    "        if(first_index < comma_index):\n",
    "            \n",
    "            detokenizer = TreebankWordDetokenizer()\n",
    "            before_first_verb = detokenizer.detokenize(tokens[:first_index])\n",
    "\n",
    "            # Approach #1 : Just copy paste everything that comes before the first verb and put it \n",
    "            # at the start of the second sentence.\n",
    "            second_sentence = before_first_verb + text.split(\",\")[1]\n",
    "            print(f\"Approach 1: {second_sentence}\")\n",
    "            \n",
    "            # Approach #2 : Test if what comes before the first verb is a proper noun (i.e. 'President Trump')\n",
    "            if(before_first_verb == split_title2[0]):\n",
    "                second_sentence = before_first_verb + text.split(\",\")[1]\n",
    "                print(f\"Approach 2: {second_sentence}\")\n",
    "            else:\n",
    "                print(\"Approach 2 Failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Biden', 'calls', 'Supreme Court', 'DACA', 'ruling', 'a', '‘victory,’', 'vows', 'to', 'make', 'program', '‘permanent’']\n"
     ]
    }
   ],
   "source": [
    "print(split_title2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit09ad96bb2f3f471a8dc4f669ef23c3db"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
